"""
Agent 8: Report Generator (MedGemma + LoRA).
Synthesizes structured radiology reports from all upstream insights.
"""
from ..base import BaseAgent
from typing import Any, Dict

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
except ImportError:
    torch = None
    AutoModelForCausalLM = None

class ReportGeneratorAgent(BaseAgent):
    def __init__(self, device="cpu"):
        super().__init__(name="Agent 8: Report Generator")
        
        if torch and torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
            
        self.tokenizer = None
        self.model = None
        
        if AutoModelForCausalLM and torch:
            print(f"[{self.name}] Loading MedGemma-2B with LoRA adapters on {self.device}...")
            # Ideally load from local path or HF Hub
            # For skeleton, we demonstrate the loading code but assume 'google/gemma-2b' exists or handles gracefully
            try:
                base_model_id = "google/gemma-2b"
                # self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
                # base_model = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=torch.float16)
                # self.model = PeftModel.from_pretrained(base_model, "path/to/lora/adapters")
                # self.model.to(self.device)
                print(f"[{self.name}] Model loading skeleton ready. (Uncomment to run with real weights)")
            except Exception as e:
                print(f"[{self.name}] Warning: Could not init model (weights missing?): {e}")
        else:
             print(f"[{self.name}] WARNING: Transformers not installed.")

    def forward(self, results_data: Dict[str, Any]) -> str:
        """
        Synthesizes structured text report.
        """
        print(f"[{self.name}] Synthesizing final report...")
        
        anatomy = results_data.get("anatomy", "Unknown location")
        pathology = results_data.get("pathology", "Unknown pathology")
        measurements = results_data.get("measurement", {})
        guidelines = results_data.get("rag", {}).get("guideline", "")
        
        # Structured Prompt
        prompt = f"""
        ## ANATOMY: {anatomy}
        ## PATHOLOGY: {pathology}
        ## MEASUREMENTS: {measurements}
        ## GUIDELINES: {guidelines}
        
        ## TASK:
        Generate a radiology report with:
        1. FINDINGS section
        2. IMPRESSION section
        """
        
        if self.model and self.tokenizer:
            # Real generation
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=200)
            report = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return report
        else:
            # Mock fallback logic for skeleton
            report = f"""
            (Generated by Skeleton Logic - Load Weights for Real Output)
            
            REPORT:
            --------------------------------------------------
            FINDINGS:
            A lesion is seen in the {anatomy}. 
            Metrics: {measurements} in size.
            Appearance consistent with {pathology}.
            
            IMPRESSION:
            1. {pathology} in {anatomy}.
            2. Recommendation: {guidelines}
            --------------------------------------------------
            """
            return report.strip()
